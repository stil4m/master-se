\subsection*{What is Fault Prediction?}

Fault prediction is a process to detect faulty parts in a piece of software.
This process helps to understand the required maintenance effort for testing and resource allocation \autocite[415]{MAKING_SOFTWARE}.

Tooling or people give an approximation of the parts of the software that are likely to have more faults.
Based on this given, the software team can decide on which parts more focus is required or on what components more resources are required \autocite[415-416]{MAKING_SOFTWARE}.
These decision must also considerate the importance of these software components in the totality of the code base.

The main benefit of fault prediction is the ability to mitigate of risks.
On the other hand, there is an investment to actually apply the risk mitigation.

\subsection*{Forms of Fault Prediction}

There are different types of fault prediction. Catal et al. classify different studies in four categories \autocite{Catal20097346}:
\begin{itemize}
\setlength\itemsep{0em}
\item The type of metrics. This covers if the metrics are applied on method, class, file, component or process level.
\item The applied method. If statistics, machine learning and/or an experts opinion is applied. These methods could be combined.
\item When the research was applied. This classification contributed to recognize if the focus on method and/or metrics are shifting.
\item The kind of dataset that was used. A dataset can be private or public or partially public. This identifies if studies are reproducible.
\end{itemize}

This classification of Catal et al. gives an overview of the performed research on the topic.
However, I find that Catal et al. make some questionable conclusions.
Since it is supposed to be a survey paper, the writer was tempted to add some, in my opinion, substantial conclusions.
For example he concludes that the higher amount of publicly used datasets \autocite[7349]{Catal20097346}
and there is less motivation to use statistically based methods but instead machine learning \autocite[7350]{Catal20097346}.
I find this rather easy, since the capabilities of researchers increased over the last years.
Hardware improved, publicly shared data sets were made easier due to companies as \textit{GitHub} \autocite{github}
and machine learning is easier to apply due to products as \textit{Tensor Flow} \autocite{abaditensorflow}.
These developments may have made these options feasible.
Finally, Catal et al. state ``as specified in this review, machine learning models have better features than statistical methods or expert opinion based approaches'' \autocite[7351]{Catal20097346}.
However, the whole paper does not show this and again this conclusion is substantial.

Another example is a claim that ``we need to increase the percentage of papers using public datasets and 80\% can be an ideal level'' \autocite[7351]{Catal20097346}.
First there is no substantial argument for the `80\%'. I agree that public data sets increase the reproducibility of the research, which should be a goal in research.
However, private data sets may be more representative since they might be significantly bigger than public software projects as they probably include years of development.
The point made, we cannot set a limit for 80\%, but we have to look for representing data that improves the field.

The \textit{Making Software} \autocite{MAKING_SOFTWARE} book has another approach. This literature focuses only on metrics, but classifies these metrics in six categories.
These are Code Coverage, Code Churn, Code Complexity, Code Dependencies, People \& Organizational Metrics and Integrated/Combined approach.
The results of this study show that Organizational Structure measures show the highest accuracy on their case \autocite[430]{MAKING_SOFTWARE}.
This result is explainable when you think about that when a lot of different minds work on a single piece of software the ideas may start to differentiate, which may lead to more bugs.
However, when you turn the idea around, the reason why different people work on the same piece of software may come from different requirements that may be contradict or interfere with each other.
The point made that the arise of this bugs may not be directly associated with different developers working together, but might originate from another part of the organization.
\\
The discussed research seems to focus on finding breakthroughs to identify what approach shows the best performance.
The focus on the conduciveness for the industry seems to be low. More on this in the \textit{Questionable Aspects}

\subsection*{When should you use Fault Prediction?}

The main benefits of Fault Prediction are \autocite[4626]{Catal20114626}:

\begin{itemize}
\setlength\itemsep{0em}
\item Reaching a highly dependable system,
\item Improving test process by focusing on fault-prone modules,
\item Selection of best design from design alternatives using object-oriented metrics,
\item Identifying refactoring candidates that are predicted as fault-prone,
\item Improving quality by improving test process.
\end{itemize}

The literature does not state when you should apply this process,
but abstracting from the benefits one can say that the intended result can be a high critical system with a low level of fault-tolerance.

This leaves me with the unanswered question, when is a system critical enough that you should apply these methods?
In the literature companies such as NASA are mentioned that apply this tooling \autocite[4626]{Catal20114626} \autocite[7347]{Catal20097346}.
The importance to mitigate risks for companies such as NASA is much higher than the software companies that build simple web services.
I can only assume that it is less likely to apply this process in environments where faults are `allowed'.
For example, software to scrape a website has much higher fault-tolerance, since it is easy to re-execute these kinds of software.

Another consideration that you should consider when using fault prediction is the possible change in resource allocation.
As mentioned in \textit{What is Fault Prediction?} the process will give you insight on parts of the software that may require more effort in maintenance.
The researched literature only states types of fault prediction and its reliability.
There is no notion of the actions that a team should apply when having the knowledge. Since it is prediction, the team only knows that bugs may occur.
Should the team focus on this prediction, or should the team focus on their current backlog, which probably include concrete bugs?
Of course, when there are no bugs and the team runs in a NASA like environment, then the focus of the team may shift or expand.
But should the team allocate new team members on mitigating the risks in these `faulty' modules?

To decide when to use fault prediction a team or company must decide if the risk mitigation is worth it.
When a risk turns in to a problem it may have a large impact. For example, a exploding space shuttle.
Not only the costs, but also people who might get hurt should be included in this consideration.

\subsection*{Questionable Aspects}

The literature left me with some questionable aspects.

\subsubsection*{Severity over Quantity}

The literature focuses on the quantity of bugs and where they are likely to occur.
The severity of a single defect might be much greater than a group of other defects.
Think about a 500 response of web server instead of the required 404 due to a NullPointerException in comparison to a security issue where a user might drop the whole database.
The first may occur in greater numbers in an API module, where the second might occur only once in a database module.
The fault prediction would push the team's focus towards the API module based on the quantity of defects, which might be less important.
In the encountered research defect severity was not taken into account, which in my opinion is an important aspect in prioritisation.

\subsubsection*{Minimum Project Size}

A lot of big data sets are used in the research of the different papers.
With the researched papers I still have no idea on what environment it is applicable to do fault detection.
What would the minimum lines of code be of the project, does it apply on green field project and/or are their even restrictions on the code base?

\subsubsection*{Combining Approaches}

Both in Catal et al. \autocite{Catal20097346} and Nachiappan et al. \autocite{MAKING_SOFTWARE}
the covered research focuses on a certain type of metrics or a fault detection level.
What remains an uncertainty is the effect of combining different metric types or fault detection levels.
For example, I am curious if it is possible to reach an higher efficiency rate when the `Organizational Structure' approach is combined with `Code Churn' or `Code Complexity'.
I question if the industry could benefit if the research was focussed on combining approaches to improve efficiency, instead of the decomposition of types that was performed in the literature.

\subsection*{Conclusion}

Fault prediction can identify parts of software that may contain a high amount of defects.
Teams that build low fault-tolerance software can benefit from this information by allocating more/other resources.
With different metrics, on different levels it is possible to show these spots with a reasonable amount of certainty.
Machine learning is a more popular method than statistical methods to detect faults \autocite[7347]{Catal20097346} and Organizational Structure seems to be a good identifier to use as metric \autocite{MAKING_SOFTWARE}.
I would only apply fault detection when I would work on a high-risk software project.
Personally I think that on other software projects it only adds more workload on the existing backlog.
